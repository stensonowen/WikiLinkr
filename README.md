# WikiLinkr
## Stores the structure of links of the entire English Wikipedia in a hash table and finds the shortest path from one to another.

### Background / Overview
The project is inspired by [the wikipedia game](https://en.wikipedia.org/wiki/Wikipedia:Wiki_Game), the goal of which is to get from one article to another only by clicking links. When I started writing it, I was mostly looking for an excuse to play around with concepts learned in classes (namely hash tables and pagefiles). I recently rewrote it to be more organized so that it would be easier to make performance-enhancing tweaks.

The parser reads in a Wikipedia dump and outputs a file containing only the article names and their links. table.h reads through that file and creates a hash map containing the hashes of articles its entry links to. (For example, the article title "DNA" might hash to 123456, so an entry containing its name would be created at table[123456]. The article for DNA might link to the article RNA, which hashes to 987654, so table[123456]->links should include 987654, and so on.) It is helpful to use a hash table because it is important that this structure scale well, as it holds tens of millions of unique articles (in the worst case). Storing hashes is also much more efficient than storing article titles, considering how many are being stored.

BFS.h performs a breadth-first between articles given their hashes. It pulls up all children of the source article looking for the destination. If it isn't there, each of these children is added to a set of seen hashes to prevent redundancy or loops, and all of their children are compared against the destination. This scales pretty terribly, considering that articles can have thousands of links. The maximum depth of a search is thus capped, but most paths can be found in fewer. Unfortunately, weighted graphs get all the fun algorithms, so there isn't much to do besides a pretty straightforward breadth-first search. 

In the future I may add some kind of cacheing to high-traffic articles. An adjustment with a clear benefit would be storing links going in both directions (i.e. not just storing all pages a given article links to, but also all pages that link to it); this would allow the source and destination nodes to branch toward each other, meaning substantially fewer fruitless paths to check. This would be expensive, though, consuming up to almost double the memory and slow down the table population (inserting hundreds of links into an entry is relatively quick because the exact number is calculated by the parser and a vector is contiguous in memory; adding a second link from the child to the parent would be expensive because it couldn't take advantage of cacheing in the same way, and the number of parents cannot be easily calculated so the vector would be constantly resized, not to mention the extra slowdown from so many more mutex locks). I initially didn't try to implement anything like this because I was very focused on reducing the memory usage (which was once almost 100G of RAM).

I was going to use a third-party parser to sort through the irrelevant noise from a Wikipedia dump (the only thing that matters is the link structure, which is about 10% of the dump),  but couldn't find any that both did what I needed and didn't crash all the time. I wrote my own parser in Python because populating the hash table from a parsed file is much less expensive than parsing a dump every time the program is run; this way a dump only needs to be parsed once. I wrote the parser in Python because it was initially going to be able to update dumps from the internet natively (and parsing also just lends itself to being written in Python), but that never happened and I never rewrote it in C++. It doesn't have to be particularly fast because it is only run once per dump (which are updated every few weeks). I may multithread the parser, but it is run so infrequently that it really doesn't matter. It takes about 10-15 minutes to parse the entire English dump (~50GB) on one core, which is very manageable (it is faster to parse the dump than to unzip it).

This program was initially extremely expensive, but I've slimmed it down quite a bit in the last few weeks. The primary difference came down to taking advantage of cacheing with spatial locality and cutting down on unnecessary pointer usage. The biggest cause of this was storing page links in a (pointer to a) linked list, which of course has non-contiguous elements and stores pointers to its next and previous nodes. By using the parser to convey the size of an article, it was easy to use a vector without worrying about resizing issues. I also multithreaded the table population process (because the bottleneck wasn't disk io, and, because I wrote the parser, it was easy to split the parsed dump into as many chunks as there are cores). This brought down the memory consumption from ~85GB to ~9, and the run-time from ~10 hours to ~1 minute (though to be fair, the early version spent so much of its time and energy swapping memory that the time comparison might be skewed). I also switched from testing it on Windows with Visual Studio to Linux (Mint Rafaela) with gcc, which sped up the simple wikipedia from 2-3 minutes to 2-3 seconds. (However, as far as I know, Windows may be more forgiving with using an irresponsible amount of RAM because it uses a pagefile, which swaps pages, while Linux seems to always use a swap file/partition, which swaps entire processes, and thus would be unhelpful if a single process had such exorbitant memory usage).

If you don't have 16+GB of RAM or you don't want to obtain the entire English wikipedia dump/link files, it is much quicker to use the collection of Simple Wikipedia articles, which is significantly (about 100x) smaller. This dump takes about 10 seconds to parse, about a second to load into a table, and uses about 300MB of RAM, making it more reasonable. Additionally, the Simple Wiki collection includes fewer articles but most common ones, which are going to be used most frequently in finding shorter paths, so the results are usually similar but much more accessible. This does mean it's impossible to search for very obscure articles, but that was impractical anyway because of how the search algorithm scales.

I thought this project lended itself well to being hosted online, because it is relatively slow to set up but quick to use. I know very little about wobsites, particularly web framework in C++, so the site is a little bit hack-ey. It's pretty much just a little bit of [crow](https://github.com/ipkn/crow), which was the most straightforward C++ web framework I could find and a tiny bit of html. I wrote/tested most of this project over ssh, so the web side of it was done using lynx, so it's a little bland. The website is really just a means of showcasing the actual program though.

### Quick Setup: 
The quickest way to get this up and running is to use the Simple Wiki xml dump. From the 'simplewiki' link [here](https://dumps.wikimedia.org/backup-index.html), download simplewiki-XXXXXXXX-pages-articles.xml.bz2, unzip it, parse it, and pass it to the main program to use.
